<!DOCTYPE html>
<html>
<head>
  <meta http-equiv="content-type" content="text/html; charset=UTF-8">
  <meta name="robots" content="noindex, nofollow">
  <meta name="googlebot" content="noindex, nofollow">

  
  

  
  
  

  

  <script type="text/javascript" src="https://code.jquery.com/jquery-3.1.0.js"></script>

  

  

  

  
  

  
    
      <script type="text/javascript" src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    
  
    
      <link rel="stylesheet" type="text/css" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap-theme.min.css">
    
  
    
      <link rel="stylesheet" type="text/css" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    
  
    
      <script type="text/javascript" src="https://download.affectiva.com/js/3.2/affdex.js"></script>
    
  

  <style type="text/css">
    
  </style>

  <title>Emotion from Camera Sample App by vradhakrishnan1</title>

  
</head>

<body>
  <body>
  <div class="container-fluid">
    <div class="row">
      <div class="col-md-8" id="affdex_elements" style="width:680px;height:480px;"></div>
      <div class="col-md-4">
        <div style="height:25em;">
          <strong>EMOTION TRACKING RESULTS</strong>
          <div id="results" style="word-wrap:break-word;"></div>
        </div>
        <div>
          <strong>DETECTOR LOG MSGS</strong>
        </div>
        <div id="logs"></div>
      </div>
    </div>
    <div>
      <button id="start" onclick="onStart()">Start</button>
      <button id="stop" onclick="onStop()">Stop</button>
      <button id="reset" onclick="onReset()">Reset</button>
      <h3>Affectiva JS SDK CameraDetector to track different emotions.</h3>
      <p>
        <strong>Instructions</strong>
        </br>
        Press the start button to start the detector.
        <br/> When a face is detected, the probabilities of the different emotions are written to the DOM.
        <br/> Press the stop button to end the detector.
      </p>
    </div>
    <div>
    	<h3>Your Query here!</h3>
    	<iframe name="resultFrame" width="1600" height="600"></iframe>
    </div>

  </div>
</body>

  




<script type='text/javascript'>//<![CDATA[

      // SDK Needs to create video and canvas nodes in the DOM in order to function
      // Here we are adding those nodes a predefined div.
      var divRoot = $("#affdex_elements")[0];
      var width = 640;
      var height = 480;
      var faceMode = affdex.FaceDetectorMode.LARGE_FACES;
      //Construct a CameraDetector and specify the image width / height and face detector mode.
      var detector = new affdex.CameraDetector(divRoot, width, height, faceMode);
      var firstTime=0;
      var requestSent=0;
      var lastTimeStamp=0;
      var connectionFlag=0;
    //	var ws;


      //Enable detection of all Expressions, Emotions and Emojis classifiers.
      detector.detectAllEmotions();
      detector.detectAllExpressions();
      detector.detectAllEmojis();
      detector.detectAllAppearance();

      //Add a callback to notify when the detector is initialized and ready for runing.
      detector.addEventListener("onInitializeSuccess", function() {
        log('#logs', "The detector reports initialized");
        //Display canvas instead of video feed because we want to draw the feature points on it
        $("#face_video_canvas").css("display", "block");
        $("#face_video").css("display", "none");
      });

      function log(node_name, msg) {
        $(node_name).append("<span>" + msg + "</span><br />")
      }

      //function executes when Start button is pushed.
      function onStart() {
        if (detector && !detector.isRunning) {
          $("#logs").html("");
          detector.start();
        }
        log('#logs', "Clicked the start button");
      }

      //function executes when the Stop button is pushed.
      function onStop() {
        log('#logs', "Clicked the stop button");
        if (detector && detector.isRunning) {
          detector.removeEventListener();
          detector.stop();
        }
      };

      //function executes when the Reset button is pushed.
      function onReset() {
        log('#logs', "Clicked the reset button");
        if (detector && detector.isRunning) {
          detector.reset();

          $('#results').html("");
        }
      };

      //Add a callback to notify when camera access is allowed
      detector.addEventListener("onWebcamConnectSuccess", function() {
        log('#logs', "Webcam access allowed");
      });

      //Add a callback to notify when camera access is denied
      detector.addEventListener("onWebcamConnectFailure", function() {
        log('#logs', "webcam denied");
        console.log("Webcam access denied");
      });

      //Add a callback to notify when detector is stopped
      detector.addEventListener("onStopSuccess", function() {
        log('#logs', "The detector reports stopped");
        $("#results").html("");
      });

      //Add a callback to receive the results from processing an image.
      //The faces object contains the list of the faces detected in an image.
      //Faces object contains probabilities for all the different expressions, emotions and appearance metrics
      detector.addEventListener("onImageResultsSuccess", function(faces, image, timestamp) {
        $('#results').html("");
        log('#results', "Timestamp: " + timestamp.toFixed(2));
        log('#results', "Number of faces found: " + faces.length);
        if (faces.length > 0) {
          log('#results', "Appearance: " + JSON.stringify(faces[0].appearance));
          log('#results', "Emotions: " + JSON.stringify(faces[0].emotions, function(key, val) {
            return val.toFixed ? Number(val.toFixed(0)) : val;
          }));
          log('#results', "Expressions: " + JSON.stringify(faces[0].expressions, function(key, val) {
            return val.toFixed ? Number(val.toFixed(0)) : val;
          }));
          log('#results', "Confused: " + JSON.stringify(faces[0].expressions.browRaise, function(key, val) {
            if(val>90)
            {
            	confusionDetected(timestamp);
            }
            return val.toFixed ? Number(val.toFixed(0)) : val;
          }));
          
          log('#results', "Emoji: " + faces[0].emojis.dominantEmoji);
          drawFeaturePoints(image, faces[0].featurePoints);

        }
      });

      var confusionDetected=function(timestamp)
      {
      	console.log("Yay");
     	$('#results').html("");
      	log('#results',"You seem to be confused!!");
      	var target = "https://en.wikipedia.org/wiki/"; //the target machine
      	var word ="man"; //the word to be searched for
      	var url = target + word;
      	
/*
      	if( connectionFlag==0){

      		// ws = new WebSocket("ws://172.28.210.59:8765");
      		ws = new WebSocket("ws://localhost:8765");
      		connectionFlag=1;
      	}
*/
      	//using post method
      	/*
      	var method="POST";
      	var postData="?"+timestamp.toFixed(2);
      	var async = true;

      	var request = new XMLHttpRequest();
      	request.onload = function () {
      		var status = request.status;
      		var data = request.responseText;

      	}

      	request.open(method,url,async);
      	request.setRequestHeader("Content-Type", "text/plain;charset=UTF-8");
      	request.send(postData);
      	*/


      	/*
      	get_url="http://172.28.213.36:8000"; //saatwik 172.28.213.36:8000 //rachit : 172.28.210.59:8080
      	var request = new XMLHttpRequest();
      	request.open('GET',get_url+"/"+"15", false);//, "/?" + timestamp);
      	request.onreadystatechange = function() {
  		if (request.readyState === 4)  { 
    		var serverResponse = request.responseText;
    		window.open(serverResponse,"response","","");
  			}
		};
      	request.send();	

      	window.open(url,"resultFrame","","");
      	window.open(timestamp.toFixed(2),"value","","");
      	setTimeout(2000);
      	*/




      	//WEBSOCKET
		 if ("WebSocket" in window)
            {
            	// alert("WebSocket is supported by your Browser!");
               //console.log("here");
               // Let us open a web socket
               var ws = new WebSocket("ws://172.28.211.16:8765");
				

                ws.onopen = function()
               {
        		console.log(parseInt(timestamp),parseInt(lastTimeStamp));
               	  if(requestSent==5){
                  // Web Socket is connected, send data using send()
                  //if (parseInt(timestamp) - parseInt(lastTimeStamp) > 5){
                  //	console.log(timestamp,lastTimeStamp);
                  ws.send(timestamp);
                  //alert(timestamp + "Timestamp is sent...");
                  lastTimeStamp = timestamp ;
                  requestSent=1;
              	}
                  
              	//}
               };
				
               ws.onmessage = function (evt) 
               { 
                  var received_msg = evt.data;
                  var searchurl = target+received_msg;
                  //alert("Message is received..." + received_msg);
                  //firstTime=1;
                  window.open(searchurl,"resultFrame","","");

               };
				
               ws.onclose = function()
               { 
                  // websocket is closed.
                  //alert("Connection is closed..."); 

               };
            }
            
            else
             {
                // The browser doesn't support WebSocket
                //alert("WebSocket NOT supported by your Browser!");
             }


      }


      //Draw the detected facial feature points on the image
      function drawFeaturePoints(img, featurePoints) {
        var contxt = $('#face_video_canvas')[0].getContext('2d');

        var hRatio = contxt.canvas.width / img.width;
        var vRatio = contxt.canvas.height / img.height;
        var ratio = Math.min(hRatio, vRatio);

        contxt.strokeStyle = "#FFFFFF";
        for (var id in featurePoints) {
          contxt.beginPath();
          contxt.arc(featurePoints[id].x,
            featurePoints[id].y, 2, 0, 2 * Math.PI);
          contxt.stroke();

        }
      }

//]]> 

</script>

  <script>
  // tell the embed parent frame the height of the content
  if (window.parent && window.parent.parent){
    window.parent.parent.postMessage(["resultsFrame", {
      height: document.body.getBoundingClientRect().height,
      slug: "mzqcoL9r"
    }], "*")
  }
</script>

</body>

</html>

